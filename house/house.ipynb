{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"house.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"uFJ2jvB1zXHJ","colab_type":"text"},"cell_type":"markdown","source":["# Kaggle Housing dataset prediction\n","\n","The following notebook is a ensembling/stacking model for predicting housing sale price for kaggle competition.\n","On the kaggle public scoreboard, the submission scored 0.12207 being around top 20% on the leader board."]},{"metadata":{"id":"rb7uzOlD_Ywq","colab_type":"code","outputId":"988f960e-741d-4f79-ca04-07032cfa5a8b","executionInfo":{"status":"ok","timestamp":1543349239712,"user_tz":300,"elapsed":1715,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"cS2dKT6O0XC9","colab_type":"text"},"cell_type":"markdown","source":["Updating and installing latest xgboost and lightgbm packages"]},{"metadata":{"id":"0eiZj-GlwrOn","colab_type":"code","outputId":"8934d799-7730-4823-d649-7fb21b2c9a37","executionInfo":{"status":"ok","timestamp":1543349243496,"user_tz":300,"elapsed":5389,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"cell_type":"code","source":["!pip install xgboost\n","!pip install lightgbm"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.7.post4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.14.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.1.0)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.19.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.14.6)\n"],"name":"stdout"}]},{"metadata":{"id":"84wep7zUAe5z","colab_type":"code","colab":{}},"cell_type":"code","source":["#read data\n","train_set = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/house/data/train-1.csv')\n","test_set = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/house/data/test-1.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"akxCNrC30ejM","colab_type":"text"},"cell_type":"markdown","source":["## Data processing and feature engineering\n","\n","Year data were categorized with different time buckets based on the mean and standard deviation. Some numeric data such as YrSold and MoSold were converted to categorical features since their values were limited to 10~20 discrete values. Furthermore, features that indicate degree of quality were all converted to numeric data. Credits to Kaggle Kernel \"Comprehensive data exploration with Python\" for data visualization/feature engineering methodology"]},{"metadata":{"id":"BUBfIHhDwqef","colab_type":"code","colab":{}},"cell_type":"code","source":["from math import isnan\n","from sklearn.preprocessing import RobustScaler\n","\n","def preprocess_data(data):\n","    description = data.describe(include='all')\n","    for column in data.columns:\n","        #if categorical data\n","        if isnan(description.loc[\"mean\"][column]):\n","            #fill missing values with top categorical value\n","            data[column] = data[column].fillna(description.loc[\"top\"][column])\n","        #bucketize timezones\n","        elif column==\"GarageYrBlt\":\n","            data[column] = pd.cut(data[column],[1894.885,1918,1941,1964,1987,2010])\n","        elif column==\"YearBuilt\":\n","            data[column] = pd.cut(data[column],[1871.862,1899.6,1927.2,1954.8,1982.4,2010])\n","        elif column==\"YearRemodAdd\":\n","            data[column] = pd.cut(data[column],[1949.94,1962.0,1974,1986,1998,2010])\n","        else:\n","        #otherwise fill missing data with mean for numeric data\n","            data[column] = data[column].fillna(description.loc[\"mean\"][column])\n","    #convert numeric data to categorical data\n","    data['MSSubClass'] = data['MSSubClass'].apply(lambda x: str(x))\n","    data['YrSold'] = data['YrSold'].apply(lambda x: str(x))\n","    data['MoSold'] = data['MoSold'].apply(lambda x: str(x))\n","    \n","    #drop irrelevant data.\n","    #Id: index\n","    #PoolQC, Street, Alley, Utilities, MiscFeature: too sparse for relevant analysis\n","    data = data.drop(['PoolQC','Street','Alley','Utilities','MiscFeature','Id'],axis=1)\n","    \n","    #convert categorical data to numberic data\n","    quality_mapping = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'NA':0}\n","    exposure_mapping = {'Gd':4,'Av':3,'Mn':2,'No':1,'NA':0}\n","    finType_mapping = {'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'NA':0}\n","    data = data.replace({'HouseStyle':\n","                            {'1Story':1,'1.5Fin':1.5,'1.5Unf':1.25,'2Story':2,'2.5Fin':2.5,'2.5Unf':2.25,'SFoyer':3,'SLvl':3},\n","                         'ExterCond':quality_mapping,\n","                         'ExterQual':quality_mapping,\n","                         'BsmtQual':quality_mapping,\n","                         'BsmtCond':quality_mapping,\n","                         'BsmtExposure':exposure_mapping,\n","                         'BsmtFinType1':finType_mapping,\n","                         'BsmtFinType2':finType_mapping,\n","                         'HeatingQC':quality_mapping,\n","                         'KitchenQual':quality_mapping,\n","                         'FireplaceQu':quality_mapping,\n","                         'GarageFinish':{'Fin':3,'RFn':2,'Unf':1,'NA':0},\n","                         'GarageQual':quality_mapping,\n","                         'GarageCond':quality_mapping,\n","                        })\n","    #one-hot encode data\n","    data = pd.get_dummies(data)        \n","    return data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QFG0TaFkw4I5","colab_type":"code","outputId":"ccb87e42-0007-44b1-82fc-78b849a0de64","executionInfo":{"status":"ok","timestamp":1543349246461,"user_tz":300,"elapsed":8340,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}},"colab":{"base_uri":"https://localhost:8080/","height":1119}},"cell_type":"code","source":["#process train and test set together\n","predictions_set = test_set.loc[:,['Id']]\n","dataset = pd.concat([train_set,test_set])\n","dataset = preprocess_data(dataset)\n","\n","#split into train and test set again\n","train_set = dataset.iloc[0:1460]\n","test_features = dataset.iloc[1460:]\n","test_features.pop('SalePrice')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       180921.19589\n","1       180921.19589\n","2       180921.19589\n","3       180921.19589\n","4       180921.19589\n","5       180921.19589\n","6       180921.19589\n","7       180921.19589\n","8       180921.19589\n","9       180921.19589\n","10      180921.19589\n","11      180921.19589\n","12      180921.19589\n","13      180921.19589\n","14      180921.19589\n","15      180921.19589\n","16      180921.19589\n","17      180921.19589\n","18      180921.19589\n","19      180921.19589\n","20      180921.19589\n","21      180921.19589\n","22      180921.19589\n","23      180921.19589\n","24      180921.19589\n","25      180921.19589\n","26      180921.19589\n","27      180921.19589\n","28      180921.19589\n","29      180921.19589\n","            ...     \n","1429    180921.19589\n","1430    180921.19589\n","1431    180921.19589\n","1432    180921.19589\n","1433    180921.19589\n","1434    180921.19589\n","1435    180921.19589\n","1436    180921.19589\n","1437    180921.19589\n","1438    180921.19589\n","1439    180921.19589\n","1440    180921.19589\n","1441    180921.19589\n","1442    180921.19589\n","1443    180921.19589\n","1444    180921.19589\n","1445    180921.19589\n","1446    180921.19589\n","1447    180921.19589\n","1448    180921.19589\n","1449    180921.19589\n","1450    180921.19589\n","1451    180921.19589\n","1452    180921.19589\n","1453    180921.19589\n","1454    180921.19589\n","1455    180921.19589\n","1456    180921.19589\n","1457    180921.19589\n","1458    180921.19589\n","Name: SalePrice, Length: 1459, dtype: float64"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"odhjy51N28Ev","colab_type":"code","colab":{}},"cell_type":"code","source":["#code for visualization. Creates giant heatmap and pairplot analysis for the train set. Commented to save computation. \n","#View corr.png and pairplot.png files in the directory for the result\n","\n","#fig, ax = plt.subplots(figsize=(100,100)) \n","#sns.heatmap(data.corr(),ax=ax,annot_kws={'size': 100}).get_figure().savefig(\"./corr.png\")\n","#g = sns.PairGrid(data)\n","#g = g.map(plt.scatter)\n","#g.savefig(\"./pairplot.png\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oE7xDYeH3YTZ","colab_type":"text"},"cell_type":"markdown","source":["## Removing Outliers\n","\n","Now that we have imputed all missing data and done some feature engineering, let's make sure that we get rid of any outliers which might undermine our model"]},{"metadata":{"id":"rpGkpIWSw8GS","colab_type":"code","colab":{}},"cell_type":"code","source":["#dictionary for outlier ranges. Outlier values determined from pairplot graph above\n","outliers = {\n","    'LotFrontage': 250,\n","    'LotArea': 15000,\n","    'BsmtFinSF1':3000,\n","    'TotalBsmtSF':4000,\n","    '1stFlrSF':4000,\n","    'LowQualFinSF':800,\n","    'WoodDeckSF':1250,\n","    'EnclosedPorch':800,\n","}\n","#remove data that are out of range\n","def remove_outlier(data):\n","    for column, drop_cutoff in outliers.items():\n","        data = data.drop(data[data[column]>drop_cutoff].index)\n","    return data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5Mw8ndemw-MH","colab_type":"code","colab":{}},"cell_type":"code","source":["#Optionally use EllipticEnvelope for outlier removal. EllipticEnvelope identifies outliers based on data's distance from mean scaled by std.\n","\n","#from sklearn.covariance import EllipticEnvelope\n","#def remove_outlier(data):\n","#    detector = EllipticEnvelope(contamination=0.05)\n","#    detector.fit(data.values)\n","#    outliers = detector.predict(data.values)\n","#    outliers_index = np.where(outliers==-1)[0]\n","#    data = data.drop(outliers_index,axis=0)\n","#    return data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DrEcMZFhxBCZ","colab_type":"code","colab":{}},"cell_type":"code","source":["#remove outliers\n","train_set = remove_outlier(train_set)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9OZgJSgD35hm","colab_type":"text"},"cell_type":"markdown","source":["## Preparing Dataset for training\n","Since our dataset is ready to go, we can normalize our data for feature scaling to improve performance and do additional feature engineering before proceeding to model selection."]},{"metadata":{"id":"uXgO6o_FxDDR","colab_type":"code","colab":{}},"cell_type":"code","source":["#use robust scaler for normalizing input and feature scaling\n","scaler = RobustScaler()\n","labels = np.log1p(train_set.pop('SalePrice').values)\n","train_set = pd.DataFrame(data=scaler.fit_transform(train_set),columns=train_set.columns)\n","test_features = scaler.fit_transform(test_features)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sxDElITOxIGh","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","\n","#use random forest algorithm to determine minor features\n","randomForest = RandomForestRegressor(bootstrap=False,max_depth=50,max_features='sqrt',min_samples_leaf=1,min_samples_split=3,n_estimators=2000)\n","randomForest.fit(train_set,labels)\n","sorted_importances = sorted(enumerate(randomForest.feature_importances_),key=lambda x: x[1])\n","features_to_drop = []\n","features_index_to_drop = []\n","\n","#drop 60 features with least feature importances\n","for i,importance in sorted_importances[0:60]:\n","    features_to_drop.append(train_set.columns[i])\n","    features_index_to_drop.append(i)\n","train_set = train_set.drop(columns=features_to_drop)\n","test_features = np.delete(test_features,features_index_to_drop,axis=1)\n","train_set.columns = list(range(0,train_set.shape[1]))\n","features = train_set.values"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A8DRk5nM8DCO","colab_type":"text"},"cell_type":"markdown","source":["##Training the model\n","\n","Since we will be stacking multiple models, we will try to choose different kinds of model raning from ensembling methods to simple linear models. We will also use XGBoost and LightGBM packages which have been known for good performance. Note that the chosen models and parameters were already tuned."]},{"metadata":{"id":"nHkwuUKcxMOw","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n","from sklearn.linear_model import ElasticNet, Lasso, LassoLarsIC\n","from sklearn.kernel_ridge import KernelRidge\n","from xgboost import XGBRegressor\n","import lightgbm as lgbm\n","\n","#uncomment if you wish to add different models, but ElasticNet, KernelRidge, and LightGBM models have shown better performance overall.\n","classifiers = {\n","    'ElasticNet':ElasticNet(alpha=0.0005,l1_ratio=0.9),\n","    #'adaBoost':AdaBoostRegressor(n_estimators=3200,learning_rate=0.01),\n","    'gradientBoosting':GradientBoostingRegressor(learning_rate=0.02,loss='huber',max_depth=3,min_samples_leaf=4,min_samples_split=9,n_estimators=3000,subsample=0.9),\n","    #'ex_tree':ExtraTreesRegressor(bootstrap=False,max_depth=50,max_features='auto',min_samples_leaf=2,min_samples_split=3,n_estimators=1800,n_jobs=4),\n","    'ridge':KernelRidge(alpha=0.1,coef0=6,degree=1,kernel='polynomial'),\n","    #'xgboost':XGBRegressor(colsample_bytree=0.3,gamma=0.1,learning_rate=0.01,max_depth=60,min_child_weight=3,n_estimators=3000,subsample=0.4),\n","    'lgbm': lgbm.sklearn.LGBMRegressor(boosting_type='goss',colsample_bytree=0.5,learning_rate=0.005,max_depth=10,min_child_samples=10,min_child_weight=7,n_estimators=2500,n_jobs=4,num_leaves=7,reg_lambda=1,subsample=1.0),\n","}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C1Lxv7JzxO5s","colab_type":"code","outputId":"ab57df0d-3503-412b-e60d-c1cb1c2ee8c5","executionInfo":{"status":"ok","timestamp":1543349268215,"user_tz":300,"elapsed":270,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"cell_type":"code","source":["#This section is for using RandomSearch and GridSearch for parameter tuning. Feel free to uncomment certain parameters for tuning it.\n","\n","#from sklearn.model_selection import KFold, RandomizedSearchCV, cross_val_score, GridSearchCV\n","#params = {\n","        #'alpha':[1.0,0.5,0.1,0.05,0.001,0.005,0.0001,0.0005,0.00001,0.00005],\n","        #'l1_ratio':[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0],\n","        #'normalize':[True,False]\n","        #'n_estimators': [400,500,600,700,900,1200,1500,1800,2000,2300,2500,3000,3200,3500,4000],\n","        #'learning_rate': [0.1,0.01,0.02,0.001,0.005,0.0001,0.0005,0.00001],\n","        #'min_child_weight': [0.00001,0.0001,0.001,0.01,0.1,1,3,5,7],\n","        #'colsample_bytree': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n","        #'max_depth': [8,9,10,15,20,30,40,50,60,70],\n","        #'depth':[3,1,2,6,4,5,7,8,9,10],\n","        #'gamma':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n","        #'num_leaves':[7,14,21,31,50,100,150,200,300,400,500,1000,1500,2000,2500,3000,3500,4000,4500],\n","        #'boosting_type':['gbdt','goss','dart'],\n","        #'min_child_samples':[5,10,15,20,25,30,40,50,70,90],\n","        #'max_features': ['auto','sqrt'],\n","        #'min_samples_split':[2,3,5,7,9,11,13,15],\n","        #'min_samples_leaf':[1,2,4,6,9,11,13,15],\n","        #'bootstrap':[True,False],\n","        #'subsample':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n","        #'reg_lambda':[1,0,0.1,0.001,0.0001,0.00001,0.000001],\n","        #'loss':['ls', 'lad', 'huber', 'quantile'],\n","        #'kernel':['rbf','linear','polynomial','sigmoid'],\n","        #'degree':[1,2,3,4,5,6,7,8,9,10],\n","        #'coef0':[1,2,3,4,5,6,7,8]\n","        #'C':[0.2,0.25,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n","        #'l2_leaf_reg':[3,1,5,10,20,30,50,70,90,100,120,150],\n","        #'iterations':[250,100,500,700,1000,1500],\n","        #'bagging_temperature':[7,14,21,31,50,100,150,200,300,400,500,1000]\n","#}\n","#model = XGBRegressor(n_jobs=4)\n","#kf = KFold(n_splits=4,shuffle=True)\n","\"\"\"\n","random_search = RandomizedSearchCV(model,\n","                                   param_distributions=params,\n","                                   n_iter=1500,\n","                                   scoring='neg_mean_squared_error',\n","                                   cv=skf.split(features,labels),\n","                                   verbose=3, \n","                                   n_jobs=4)\n","\"\"\"\n","#random_search = GridSearchCV(model,param_grid=params,cv=kf.split(features,labels),verbose=3,n_jobs=4)\n","#random_search.fit(features,labels)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nrandom_search = RandomizedSearchCV(model,\\n                                   param_distributions=params,\\n                                   n_iter=1500,\\n                                   scoring='neg_mean_squared_error',\\n                                   cv=skf.split(features,labels),\\n                                   verbose=3, \\n                                   n_jobs=4)\\n\""]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"DerlWzGIxoqO","colab_type":"code","outputId":"c4598d91-3167-4191-d47e-050e841512ab","executionInfo":{"status":"ok","timestamp":1543349272116,"user_tz":300,"elapsed":398,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["#print parameter search results\n","\n","\"\"\"\n","print(random_search.cv_results_)\n","print(random_search.best_estimator_.score(features,labels))\n","print(random_search.best_score_)\n","print(random_search.best_estimator_)\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nprint(random_search.cv_results_)\\nprint(random_search.best_estimator_.score(features,labels))\\nprint(random_search.best_score_)\\nprint(random_search.best_estimator_)\\n'"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"A_mqPvLI8-Kc","colab_type":"text"},"cell_type":"markdown","source":["Before getting our first round of base model predictions, let's make sure that these are good models to try out using KFold validation. Note that Kaggle has a seperate dataset for just scoring the submissions so using heavy validation methods such as KFold validation is highly recommended."]},{"metadata":{"id":"u-UQ3QcSxsT1","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import KFold\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import cross_val_score, train_test_split\n","kf = KFold(n_splits=4,shuffle=True)\n","\n","#get KFold validation results\n","def get_score(model):\n","    score = np.sqrt(-cross_val_score(model,features,labels,scoring=\"neg_mean_squared_error\",cv=kf))\n","    return score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Fvf4WWMwxvUY","colab_type":"code","outputId":"2c94e97f-86ee-4d27-9369-7cc5c2382e78","executionInfo":{"status":"ok","timestamp":1543349376248,"user_tz":300,"elapsed":100558,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"cell_type":"code","source":["for key, classifier in classifiers.items():\n","    print(key,get_score(classifier).mean())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ElasticNet 0.11395888261874398\n","gradientBoosting 0.11284385430876491\n","ridge 0.11305002544611711\n","lgbm 0.11930848511827917\n"],"name":"stdout"}]},{"metadata":{"id":"fvJi5FJP9gPS","colab_type":"text"},"cell_type":"markdown","source":["The results seem really promissing with all of the validation loss lower than 0.12 benchmark."]},{"metadata":{"id":"DP_le15c9p1U","colab_type":"text"},"cell_type":"markdown","source":["##Base model predictions\n","\n","Here, we will use each models to print 1/n of the predictions of the whole dataset using KFold. The base model predictions on train set will be concatenated for our second round of predictions with stacking. The test set predictions will be averaged for prediciton on the second round as well. View **xgboost-house** notebook to see how stacking is done with XGBoost model."]},{"metadata":{"id":"HWa9SNXVxxhG","colab_type":"code","outputId":"c35dfd1d-97ac-4acd-b813-a46808379638","executionInfo":{"status":"ok","timestamp":1543349457556,"user_tz":300,"elapsed":26451,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}},"colab":{"base_uri":"https://localhost:8080/","height":159}},"cell_type":"code","source":["for i,(train_index,test_index) in enumerate(kf.split(features)):\n","    keys = list(classifiers.keys())\n","    classifier = classifiers[keys[i]]\n","    X_train, y_train = features[train_index], labels[train_index]\n","    X_test, y_test = features[test_index], labels[test_index]\n","    classifier.fit(X_train,y_train)\n","    \n","    #get model score and loss for the fold\n","    print(keys[i],np.sqrt(mean_squared_error(classifier.predict(X_test),y_test)))\n","    print(keys[i],classifier.score(X_test,y_test))\n","    \n","    #get features for stacking\n","    stacking_features = classifier.predict(X_test)\n","    stacking_set = pd.DataFrame({'Id':test_index})\n","    stacking_set = stacking_set.assign(SalePrice=pd.Series(stacking_features))\n","    stacking_set.to_csv('/content/gdrive/My Drive/Colab Notebooks/house/stacking_model/'+keys[i]+'.csv',index=False)\n","    \n","    #get predictions from test_features\n","    predictions = classifier.predict(test_features)\n","    predictions_set = predictions_set.assign(SalePrice=pd.Series(predictions))\n","    predictions_set.to_csv('/content/gdrive/My Drive/Colab Notebooks/house/saved_model/'+keys[i]+'.csv',index=False)\n","#save labels for later reference\n","pd.Series(labels).to_csv('/content/gdrive/My Drive/Colab Notebooks/house/saved_model/labels.csv',index=False,header=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ElasticNet 0.1024708319306177\n","ElasticNet 0.9344335732239912\n","gradientBoosting 0.12812819743845516\n","gradientBoosting 0.8806605759924498\n","ridge 0.10527682928125777\n","ridge 0.919638729597537\n","lgbm 0.1201408254126671\n","lgbm 0.9117547586448226\n"],"name":"stdout"}]}]}