{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"titanic.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"Q3aNWl1CGTdh","colab_type":"text"},"cell_type":"markdown","source":["#Kaggle Titanic Dataset\n","\n","The following is an implementation of titanic classification challenge solution using stacking. This model scored 80% accuracy in the competition which put it at top 10% on the leaderboard. In this notebook, we will explore feature enginnering methods and stacking methods"]},{"metadata":{"id":"u44bUU0_CQbq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"67912fea-bd99-427e-b158-15360087e263","executionInfo":{"status":"ok","timestamp":1543354030441,"user_tz":300,"elapsed":915,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}}},"cell_type":"code","source":["import pandas as pd\n","import shutil\n","import numpy as np\n","import re\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","#import dataset\n","train_set = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/titanic/data/train.csv')\n","test_set = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/titanic/data/test.csv')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"JJDTpWitGXNz","colab_type":"text"},"cell_type":"markdown","source":["##Preprocessing  dataset\n","\n","For the titanic dataset we will extract titles in people names to determine their social status as well as using Pclass feature. Numeric/continous data are bucketized into different ranges as categoricla inputs and synthetic features such as family size and IsAlone features are created from SibSp and Parch features. Credits on feature engineering methodologies to Kaggle Kernel \"EDA to Predict\""]},{"metadata":{"id":"fK9F80DhEaGL","colab_type":"code","colab":{}},"cell_type":"code","source":["def preprocess_data(data):\n","    def get_init(name):\n","        return re.search('([A-Za-z]+)\\.',name).group(1)\n","    description = data.describe(include = 'all')\n","    data['Embarked'] = train_set['Embarked'].fillna('S')\n","    data['Age'] = data['Age'].fillna(description.loc['mean']['Age'])\n","    data['Age'] = pd.cut(data['Age'],5)\n","    data['Fare'] = data['Fare'].fillna(description.loc['mean']['Fare'])\n","    data['Fare'] = pd.qcut(data['Fare'],4)\n","    data['Pclass'] = data['Pclass'].apply(lambda x: str(x))\n","    data['FamilySize'] = data.apply(lambda x: x['SibSp'] + x['Parch'] + 1,axis=1)\n","    data['IsAlone'] = data.apply(lambda x: 1 if x['FamilySize']==1 else 0,axis=1)\n","    data['Name'] = data['Name'].apply(get_init)\n","    data['Name'] = data['Name'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'])\n","    \n","    return data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IrWTVUChEclF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3395},"outputId":"edbfeded-e54b-4450-8544-e917687fa5a9","executionInfo":{"status":"ok","timestamp":1543354060435,"user_tz":300,"elapsed":360,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}}},"cell_type":"code","source":["#check dataset\n","train_set = preprocess_data(train_set)\n","print(train_set)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["     PassengerId  Survived Pclass    Name     Sex               Age  SibSp  \\\n","0              1         0      3      Mr    male  (16.336, 32.252]      1   \n","1              2         1      1     Mrs  female  (32.252, 48.168]      1   \n","2              3         1      3    Miss  female  (16.336, 32.252]      0   \n","3              4         1      1     Mrs  female  (32.252, 48.168]      1   \n","4              5         0      3      Mr    male  (32.252, 48.168]      0   \n","5              6         0      3      Mr    male  (16.336, 32.252]      0   \n","6              7         0      1      Mr    male  (48.168, 64.084]      0   \n","7              8         0      3  Master    male    (0.34, 16.336]      3   \n","8              9         1      3     Mrs  female  (16.336, 32.252]      0   \n","9             10         1      2     Mrs  female    (0.34, 16.336]      1   \n","10            11         1      3    Miss  female    (0.34, 16.336]      1   \n","11            12         1      1    Miss  female  (48.168, 64.084]      0   \n","12            13         0      3      Mr    male  (16.336, 32.252]      0   \n","13            14         0      3      Mr    male  (32.252, 48.168]      1   \n","14            15         0      3    Miss  female    (0.34, 16.336]      0   \n","15            16         1      2     Mrs  female  (48.168, 64.084]      0   \n","16            17         0      3  Master    male    (0.34, 16.336]      4   \n","17            18         1      2      Mr    male  (16.336, 32.252]      0   \n","18            19         0      3     Mrs  female  (16.336, 32.252]      1   \n","19            20         1      3     Mrs  female  (16.336, 32.252]      0   \n","20            21         0      2      Mr    male  (32.252, 48.168]      0   \n","21            22         1      2      Mr    male  (32.252, 48.168]      0   \n","22            23         1      3    Miss  female    (0.34, 16.336]      0   \n","23            24         1      1      Mr    male  (16.336, 32.252]      0   \n","24            25         0      3    Miss  female    (0.34, 16.336]      3   \n","25            26         1      3     Mrs  female  (32.252, 48.168]      1   \n","26            27         0      3      Mr    male  (16.336, 32.252]      0   \n","27            28         0      1      Mr    male  (16.336, 32.252]      3   \n","28            29         1      3    Miss  female  (16.336, 32.252]      0   \n","29            30         0      3      Mr    male  (16.336, 32.252]      0   \n","..           ...       ...    ...     ...     ...               ...    ...   \n","861          862         0      2      Mr    male  (16.336, 32.252]      1   \n","862          863         1      1     Mrs  female  (32.252, 48.168]      0   \n","863          864         0      3    Miss  female  (16.336, 32.252]      8   \n","864          865         0      2      Mr    male  (16.336, 32.252]      0   \n","865          866         1      2     Mrs  female  (32.252, 48.168]      0   \n","866          867         1      2    Miss  female  (16.336, 32.252]      1   \n","867          868         0      1      Mr    male  (16.336, 32.252]      0   \n","868          869         0      3      Mr    male  (16.336, 32.252]      0   \n","869          870         1      3  Master    male    (0.34, 16.336]      1   \n","870          871         0      3      Mr    male  (16.336, 32.252]      0   \n","871          872         1      1     Mrs  female  (32.252, 48.168]      1   \n","872          873         0      1      Mr    male  (32.252, 48.168]      0   \n","873          874         0      3      Mr    male  (32.252, 48.168]      0   \n","874          875         1      2     Mrs  female  (16.336, 32.252]      1   \n","875          876         1      3    Miss  female    (0.34, 16.336]      0   \n","876          877         0      3      Mr    male  (16.336, 32.252]      0   \n","877          878         0      3      Mr    male  (16.336, 32.252]      0   \n","878          879         0      3      Mr    male  (16.336, 32.252]      0   \n","879          880         1      1     Mrs  female  (48.168, 64.084]      0   \n","880          881         1      2     Mrs  female  (16.336, 32.252]      0   \n","881          882         0      3      Mr    male  (32.252, 48.168]      0   \n","882          883         0      3    Miss  female  (16.336, 32.252]      0   \n","883          884         0      2      Mr    male  (16.336, 32.252]      0   \n","884          885         0      3      Mr    male  (16.336, 32.252]      0   \n","885          886         0      3     Mrs  female  (32.252, 48.168]      0   \n","886          887         0      2   Other    male  (16.336, 32.252]      0   \n","887          888         1      1    Miss  female  (16.336, 32.252]      0   \n","888          889         0      3    Miss  female  (16.336, 32.252]      1   \n","889          890         1      1      Mr    male  (16.336, 32.252]      0   \n","890          891         0      3      Mr    male  (16.336, 32.252]      0   \n","\n","     Parch            Ticket             Fare        Cabin Embarked  \\\n","0        0         A/5 21171   (-0.001, 7.91]          NaN        S   \n","1        0          PC 17599  (31.0, 512.329]          C85        C   \n","2        0  STON/O2. 3101282   (7.91, 14.454]          NaN        S   \n","3        0            113803  (31.0, 512.329]         C123        S   \n","4        0            373450   (7.91, 14.454]          NaN        S   \n","5        0            330877   (7.91, 14.454]          NaN        Q   \n","6        0             17463  (31.0, 512.329]          E46        S   \n","7        1            349909   (14.454, 31.0]          NaN        S   \n","8        2            347742   (7.91, 14.454]          NaN        S   \n","9        0            237736   (14.454, 31.0]          NaN        C   \n","10       1           PP 9549   (14.454, 31.0]           G6        S   \n","11       0            113783   (14.454, 31.0]         C103        S   \n","12       0         A/5. 2151   (7.91, 14.454]          NaN        S   \n","13       5            347082  (31.0, 512.329]          NaN        S   \n","14       0            350406   (-0.001, 7.91]          NaN        S   \n","15       0            248706   (14.454, 31.0]          NaN        S   \n","16       1            382652   (14.454, 31.0]          NaN        Q   \n","17       0            244373   (7.91, 14.454]          NaN        S   \n","18       0            345763   (14.454, 31.0]          NaN        S   \n","19       0              2649   (-0.001, 7.91]          NaN        C   \n","20       0            239865   (14.454, 31.0]          NaN        S   \n","21       0            248698   (7.91, 14.454]          D56        S   \n","22       0            330923   (7.91, 14.454]          NaN        Q   \n","23       0            113788  (31.0, 512.329]           A6        S   \n","24       1            349909   (14.454, 31.0]          NaN        S   \n","25       5            347077  (31.0, 512.329]          NaN        S   \n","26       0              2631   (-0.001, 7.91]          NaN        C   \n","27       2             19950  (31.0, 512.329]  C23 C25 C27        S   \n","28       0            330959   (-0.001, 7.91]          NaN        Q   \n","29       0            349216   (-0.001, 7.91]          NaN        S   \n","..     ...               ...              ...          ...      ...   \n","861      0             28134   (7.91, 14.454]          NaN        S   \n","862      0             17466   (14.454, 31.0]          D17        S   \n","863      2          CA. 2343  (31.0, 512.329]          NaN        S   \n","864      0            233866   (7.91, 14.454]          NaN        S   \n","865      0            236852   (7.91, 14.454]          NaN        S   \n","866      0     SC/PARIS 2149   (7.91, 14.454]          NaN        C   \n","867      0          PC 17590  (31.0, 512.329]          A24        S   \n","868      0            345777   (7.91, 14.454]          NaN        S   \n","869      1            347742   (7.91, 14.454]          NaN        S   \n","870      0            349248   (-0.001, 7.91]          NaN        S   \n","871      1             11751  (31.0, 512.329]          D35        S   \n","872      0               695   (-0.001, 7.91]  B51 B53 B55        S   \n","873      0            345765   (7.91, 14.454]          NaN        S   \n","874      0         P/PP 3381   (14.454, 31.0]          NaN        C   \n","875      0              2667   (-0.001, 7.91]          NaN        C   \n","876      0              7534   (7.91, 14.454]          NaN        S   \n","877      0            349212   (-0.001, 7.91]          NaN        S   \n","878      0            349217   (-0.001, 7.91]          NaN        S   \n","879      1             11767  (31.0, 512.329]          C50        C   \n","880      1            230433   (14.454, 31.0]          NaN        S   \n","881      0            349257   (-0.001, 7.91]          NaN        S   \n","882      0              7552   (7.91, 14.454]          NaN        S   \n","883      0  C.A./SOTON 34068   (7.91, 14.454]          NaN        S   \n","884      0   SOTON/OQ 392076   (-0.001, 7.91]          NaN        S   \n","885      5            382652   (14.454, 31.0]          NaN        Q   \n","886      0            211536   (7.91, 14.454]          NaN        S   \n","887      0            112053   (14.454, 31.0]          B42        S   \n","888      2        W./C. 6607   (14.454, 31.0]          NaN        S   \n","889      0            111369   (14.454, 31.0]         C148        C   \n","890      0            370376   (-0.001, 7.91]          NaN        Q   \n","\n","     FamilySize  IsAlone  \n","0             2        0  \n","1             2        0  \n","2             1        1  \n","3             2        0  \n","4             1        1  \n","5             1        1  \n","6             1        1  \n","7             5        0  \n","8             3        0  \n","9             2        0  \n","10            3        0  \n","11            1        1  \n","12            1        1  \n","13            7        0  \n","14            1        1  \n","15            1        1  \n","16            6        0  \n","17            1        1  \n","18            2        0  \n","19            1        1  \n","20            1        1  \n","21            1        1  \n","22            1        1  \n","23            1        1  \n","24            5        0  \n","25            7        0  \n","26            1        1  \n","27            6        0  \n","28            1        1  \n","29            1        1  \n","..          ...      ...  \n","861           2        0  \n","862           1        1  \n","863          11        0  \n","864           1        1  \n","865           1        1  \n","866           2        0  \n","867           1        1  \n","868           1        1  \n","869           3        0  \n","870           1        1  \n","871           3        0  \n","872           1        1  \n","873           1        1  \n","874           2        0  \n","875           1        1  \n","876           1        1  \n","877           1        1  \n","878           1        1  \n","879           2        0  \n","880           2        0  \n","881           1        1  \n","882           1        1  \n","883           1        1  \n","884           1        1  \n","885           6        0  \n","886           1        1  \n","887           1        1  \n","888           4        0  \n","889           1        1  \n","890           1        1  \n","\n","[891 rows x 14 columns]\n"],"name":"stdout"}]},{"metadata":{"id":"tyIKBqvBEgfz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"31ee512e-a433-4886-98b2-7fb4457ef5eb","executionInfo":{"status":"ok","timestamp":1543354064828,"user_tz":300,"elapsed":338,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}}},"cell_type":"code","source":["#We will select only these features because features such as ticket only contain strings with unknown meanings\n","features = train_set[['Name','Pclass','SibSp','Parch','Sex','Age','FamilySize','Fare','Embarked','IsAlone']]\n","features = pd.get_dummies(features)\n","print(features.columns)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Index(['SibSp', 'Parch', 'FamilySize', 'IsAlone', 'Name_Master', 'Name_Miss',\n","       'Name_Mr', 'Name_Mrs', 'Name_Other', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n","       'Sex_female', 'Sex_male', 'Age_(0.34, 16.336]', 'Age_(16.336, 32.252]',\n","       'Age_(32.252, 48.168]', 'Age_(48.168, 64.084]', 'Age_(64.084, 80.0]',\n","       'Fare_(-0.001, 7.91]', 'Fare_(7.91, 14.454]', 'Fare_(14.454, 31.0]',\n","       'Fare_(31.0, 512.329]', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n","      dtype='object')\n"],"name":"stdout"}]},{"metadata":{"id":"MYAOpAx6HjDc","colab_type":"text"},"cell_type":"markdown","source":["##Training base models\n","\n","For stacking methods, we will use various models and learners to get our first prediction and use the output as features of the second round. First, let us choose models that perform well in our first stage. Note that models and their parameters were already chosen from parameters search"]},{"metadata":{"id":"3RUU4IEHElVz","colab_type":"code","colab":{}},"cell_type":"code","source":["#get labels\n","labels = train_set.pop('Survived')\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","\n","#fine-tuned models\n","classifiers = {\n","    'random_forest':RandomForestClassifier(n_estimators=3000,max_depth=20,max_features='auto',min_samples_leaf=2,min_samples_split=5,oob_score = True),\n","    'adaBoost':AdaBoostClassifier(n_estimators=500,learning_rate=0.05),\n","    'gradientBoosting':GradientBoostingClassifier(learning_rate=0.1,max_depth=3,max_features='sqrt',n_estimators=500,min_samples_split=4,min_samples_leaf=2),\n","    'svc':SVC(kernel='linear',C=0.5,gamma='auto',degree=1),\n","    'ex_tree':ExtraTreesClassifier(max_depth=12,max_features='sqrt',min_samples_leaf=1,min_samples_split=10,oob_score=True,bootstrap=True,n_estimators=500)\n","}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aFxojmM4EmZQ","colab_type":"code","colab":{}},"cell_type":"code","source":["#prepare test features \n","test_set = preprocess_data(test_set)\n","test_features = test_set[['Name','Pclass','SibSp','Parch','Sex','Age','FamilySize','Fare','Embarked','IsAlone']]\n","test_features = pd.get_dummies(test_features)\n","test_features = test_features.drop(['Name_Dona'],axis=1)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CnpsjKEvEo2h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":728},"outputId":"f4890ebc-1273-4e92-c406-a27861152810","executionInfo":{"status":"ok","timestamp":1543354176469,"user_tz":300,"elapsed":55033,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}}},"cell_type":"code","source":["#use KFold validation for getting results\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","kf = KFold(n_splits=7,shuffle=True)\n","for key, classifier in classifiers.items():\n","    for train_index,test_index in kf.split(features):\n","        X_train, y_train = features.iloc[train_index], labels.iloc[train_index]\n","        X_test, y_test = features.iloc[test_index], labels.iloc[test_index]\n","        classifier.fit(X_train,y_train)\n","        print(key,accuracy_score(classifier.predict(features),labels))\n","    #print overall performance\n","    print(\"overall\",key,accuracy_score(classifier.predict(features),labels))\n","    #get predictions from test_features\n","    predictions = classifier.predict(test_features)\n","    predictions_set = test_set.loc[:,['PassengerId']]\n","    predictions_set = predictions_set.assign(Survived=pd.Series(predictions))\n","    predictions_set.to_csv('/content/gdrive/My Drive/Colab Notebooks/titanic/saved_model/'+key+'.csv',index=False)\n","    #get stacking features from training set\n","    stacking_features = classifier.predict(features)\n","    stacking_set = train_set.loc[:,['PassengerId']]\n","    stacking_set = stacking_set.assign(Survived=pd.Series(stacking_features))\n","    stacking_set.to_csv('/content/gdrive/My Drive/Colab Notebooks/titanic/stacking_model/'+key+'.csv',index=False)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["random_forest 0.8619528619528619\n","random_forest 0.8585858585858586\n","random_forest 0.8608305274971941\n","random_forest 0.8585858585858586\n","random_forest 0.8653198653198653\n","random_forest 0.8608305274971941\n","random_forest 0.8585858585858586\n","overall random_forest 0.8585858585858586\n","adaBoost 0.8237934904601572\n","adaBoost 0.8260381593714927\n","adaBoost 0.8271604938271605\n","adaBoost 0.8249158249158249\n","adaBoost 0.8282828282828283\n","adaBoost 0.8282828282828283\n","adaBoost 0.8260381593714927\n","overall adaBoost 0.8260381593714927\n","gradientBoosting 0.8765432098765432\n","gradientBoosting 0.8698092031425365\n","gradientBoosting 0.8720538720538721\n","gradientBoosting 0.8698092031425365\n","gradientBoosting 0.8731762065095399\n","gradientBoosting 0.8754208754208754\n","gradientBoosting 0.8787878787878788\n","overall gradientBoosting 0.8787878787878788\n","svc 0.8249158249158249\n","svc 0.8226711560044894\n","svc 0.8249158249158249\n","svc 0.8260381593714927\n","svc 0.8260381593714927\n","svc 0.8226711560044894\n","svc 0.8282828282828283\n","overall svc 0.8282828282828283\n","ex_tree 0.8608305274971941\n","ex_tree 0.856341189674523\n","ex_tree 0.8641975308641975\n","ex_tree 0.8597081930415263\n","ex_tree 0.8608305274971941\n","ex_tree 0.8641975308641975\n","ex_tree 0.867564534231201\n","overall ex_tree 0.867564534231201\n"],"name":"stdout"}]},{"metadata":{"id":"Alo2puOsIP3a","colab_type":"text"},"cell_type":"markdown","source":["The overall result from the KFold validation seems very promising with over 80% accuracy for most models. Now, let's proceed to the second stage with **xgboost-titanic** notebook for stacking methods with XGBoost Regressor"]}]}